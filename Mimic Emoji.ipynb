{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import requests\n",
    "import cv2\n",
    "import operator\n",
    "import numpy as np\n",
    "from __future__ import print_function\n",
    "import random\n",
    "\n",
    "from helpers import imshow\n",
    "\n",
    "# Import library to display results\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "# Display images within Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "\n",
    "_url = 'https://westus.api.cognitive.microsoft.com/emotion/v1.0/recognize'\n",
    "_key = \"7fdfca28bafc47919ea6a1d3131e8fc0\" #Here you have to paste your primary key\n",
    "_maxNumRetries = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processRequest( json, data, headers, params ):\n",
    "\n",
    "    \"\"\"\n",
    "    Helper function to process the request to Project Oxford\n",
    "\n",
    "    Parameters:\n",
    "    json: Used when processing images from its URL. See API Documentation\n",
    "    data: Used when processing image read from disk. See API Documentation\n",
    "    headers: Used to pass the key information and the data type request\n",
    "    \"\"\"\n",
    "\n",
    "    retries = 0\n",
    "    result = None\n",
    "\n",
    "    while True:\n",
    "\n",
    "        response = requests.request( 'post', _url, json = json, data = data, headers = headers, params = params )\n",
    "\n",
    "        if response.status_code == 429: \n",
    "\n",
    "            print( \"Message: %s\" % ( response.json()['error']['message'] ) )\n",
    "\n",
    "            if retries <= _maxNumRetries: \n",
    "                time.sleep(1) \n",
    "                retries += 1\n",
    "                continue\n",
    "            else: \n",
    "                print( 'Error: failed after retrying!' )\n",
    "                break\n",
    "\n",
    "        elif response.status_code == 200 or response.status_code == 201:\n",
    "\n",
    "            if 'content-length' in response.headers and int(response.headers['content-length']) == 0: \n",
    "                result = None \n",
    "            elif 'content-type' in response.headers and isinstance(response.headers['content-type'], str): \n",
    "                if 'application/json' in response.headers['content-type'].lower(): \n",
    "                    result = response.json() if response.content else None \n",
    "                elif 'image' in response.headers['content-type'].lower(): \n",
    "                    result = response.content\n",
    "        else:\n",
    "            print( \"Error code: %d\" % ( response.status_code ) )\n",
    "            print( \"Message: %s\" % ( response.json()['error']['message'] ) )\n",
    "\n",
    "        break\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "def getEmotion( result, img ):\n",
    "    \"\"\"Display the obtained results onto the input image\"\"\"\n",
    "        \n",
    "    for currFace in result:\n",
    "        faceRectangle = currFace['faceRectangle']\n",
    "        currEmotion = max(currFace['scores'].items(), key=operator.itemgetter(1))[0]\n",
    "        \n",
    "    return currEmotion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo Game: Mimic Emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method that imports a library of emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "def importEmoji():\n",
    "    emoji_dict = {}\n",
    "    images = glob.glob('emoji_images/*.jpg')    \n",
    "    for fname in images:\n",
    "        name = fname.split(\"/\")[-1].split(\".\")[0]\n",
    "        emoji_img = cv2.imread(fname, -1)\n",
    "        emoji_img = convert_to_bgra(emoji_img)\n",
    "        emoji_dict[name] = emoji_img\n",
    "    return emoji_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method that imports utility images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importUtil():\n",
    "    util_dict = {}\n",
    "    images = glob.glob('util_img/*.jpg')    \n",
    "    for fname in images:\n",
    "        name = fname.split(\"/\")[-1].split(\".\")[0]\n",
    "        util_img = cv2.imread(fname)\n",
    "        util_img = convert_to_bgra(util_img)\n",
    "        util_dict[name] = util_img\n",
    "    return util_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method that converts image from BGR to BGRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method that converts image from BGR to BGRA\n",
    "def convert_to_bgra(img):\n",
    "    # If image is already BGRA, just return it\n",
    "    if img.shape[2] == 4:\n",
    "        return img\n",
    "    \n",
    "    # Subtract emoji contour\n",
    "    mask = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    ret, mask = cv2.threshold(mask, 230, 255, cv2.THRESH_BINARY_INV)\n",
    "    _, contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    areas = [cv2.contourArea(cont) for cont in contours] # Calculate areas\n",
    "    cnt = contours[np.argmax(areas)] # Only keep the largest contour\n",
    "\n",
    "    mask_cnt = np.zeros(img.shape[:2], np.uint8)\n",
    "    cv2.drawContours(mask_cnt, [cnt], -1, 255, -1)\n",
    "\n",
    "    # Convert to BGRA\n",
    "    b_channel, g_channel, r_channel = cv2.split(img)\n",
    "    alpha_channel = mask_cnt\n",
    "    img_BGRA = cv2.merge((b_channel, g_channel, r_channel, alpha_channel))\n",
    "    return img_BGRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method that augments image on the video frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment image to a frame\n",
    "def augment_on_frame(frame, img, x_offset, y_offset, x_size, y_size):\n",
    "    # set size\n",
    "    img = cv2.resize(img, (x_size,y_size), interpolation = cv2.INTER_AREA)\n",
    "  \n",
    "    # set offset\n",
    "    y1, y2 = y_offset, y_offset + img.shape[0]\n",
    "    x1, x2 = x_offset, x_offset + img.shape[1]\n",
    "\n",
    "    alpha_s = img[:, :, 3] / 255.0\n",
    "    alpha_l = 1.0 - alpha_s\n",
    "    try:\n",
    "        for c in range(0, 3):\n",
    "            frame[y1:y2, x1:x2, c] = (alpha_s * img[:, :, c] +\n",
    "                                      alpha_l * frame[y1:y2, x1:x2, c])\n",
    "    except:\n",
    "        return None\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game - version1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camera = cv2.VideoCapture(0)\n",
    "# reduce frame size to speed it up\n",
    "w = 640\n",
    "camera.set(cv2.CAP_PROP_FRAME_WIDTH, w) \n",
    "camera.set(cv2.CAP_PROP_FRAME_HEIGHT, w * 3/4) \n",
    "camera.set(cv2.CAP_PROP_EXPOSURE,-4) \n",
    "camera.set(cv2.CAP_PROP_FPS, 24)\n",
    "\n",
    "# Import emoji\n",
    "emoji_dict = importEmoji()\n",
    "current_emoji = random.choice(emoji_dict.keys())\n",
    "\n",
    "# Import utility images\n",
    "util_dict = importUtil()\n",
    "\n",
    "# Initialize score and counter for displaying result information\n",
    "i = 0\n",
    "score = 0\n",
    "\n",
    "while True:\n",
    "    i -= 1\n",
    "    if i <= 0:\n",
    "        flag = False\n",
    "    \n",
    "    \n",
    "    # Get frame at flip it\n",
    "    ret, frame = camera.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    # Augment frame with emoji\n",
    "    emoji_img = emoji_dict[current_emoji]\n",
    "    augment_on_frame(frame, emoji_img, 10, 10, 150, 150)\n",
    "    \n",
    "    if cv2.waitKey(5) == 32:\n",
    "        # If space bar is pressed, extract imotion\n",
    "        bytes_string = cv2.imencode('.jpg', frame)[1].tostring()\n",
    "        headers = dict()\n",
    "        headers['Ocp-Apim-Subscription-Key'] = _key\n",
    "        headers['Content-Type'] = 'application/octet-stream'\n",
    "        json = None\n",
    "        params = None\n",
    "        result = processRequest( json, bytes_string, headers, params )\n",
    "        \n",
    "        if result is not None and len(result) > 0:    \n",
    "            # Detect Emoji\n",
    "            data8uint = np.fromstring( bytes_string, np.uint8 ) # Convert string to an unsigned int array\n",
    "            img = cv2.cvtColor( cv2.imdecode( data8uint, cv2.IMREAD_COLOR ), cv2.COLOR_BGR2RGB )\n",
    "            detected_emotion = getEmotion( result, frame )\n",
    "                        \n",
    "            # Check if emoji is matched\n",
    "            if detected_emotion == current_emoji:\n",
    "                guess = \"Correct!\"\n",
    "                correctness_img = util_dict[\"correct\"]\n",
    "                score += 1\n",
    "            else:\n",
    "                guess = \"Incorrect!\"\n",
    "                correctness_img = util_dict[\"incorrect\"]\n",
    "\n",
    "            # Display new target emoji\n",
    "        \n",
    "            flag = True\n",
    "            current_emoji = random.choice(emoji_dict.keys())\n",
    "            i = 100\n",
    "\n",
    "    elif cv2.waitKey(5) == 27:\n",
    "        break  \n",
    "    \n",
    "    # Add score to the screen\n",
    "    cv2.putText(frame,\"Score: {}\".format(score), (450,50), cv2.FONT_HERSHEY_SIMPLEX, 1,(255,0,0),6)\n",
    "        \n",
    "    if flag == True:\n",
    "        cv2.putText(frame,\"Detected: {}\".format(detected_emotion), (320,300), cv2.FONT_HERSHEY_SIMPLEX, 1,(255,0,0),4)\n",
    "        augment_on_frame(frame, correctness_img, 400, 310, 120, 120)    \n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    \n",
    "cv2.destroyAllWindows()\n",
    "camera.release()\n",
    "cv2.waitKey(1) # extra waitKey sometimes needed to close camera window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game - version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "camera = cv2.VideoCapture(0)\n",
    "# reduce frame size to speed it up\n",
    "w = 640\n",
    "camera.set(cv2.CAP_PROP_FRAME_WIDTH, w) \n",
    "camera.set(cv2.CAP_PROP_FRAME_HEIGHT, w * 3/4) \n",
    "camera.set(cv2.CAP_PROP_EXPOSURE,-4) \n",
    "camera.set(cv2.CAP_PROP_FPS, 24)\n",
    "\n",
    "# Import emoji\n",
    "emoji_dict = importEmoji()\n",
    "current_emoji = random.choice(emoji_dict.keys())\n",
    "\n",
    "# Import utility images\n",
    "util_dict = importUtil()\n",
    "\n",
    "# Import haar cascate for face detection\n",
    "face_cascade = cv2.CascadeClassifier('/usr/local/Cellar/opencv/3.4.0/share/OpenCV/haarcascades/haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Initialize score and counter for displaying result information\n",
    "i = 0\n",
    "score = 0\n",
    "\n",
    "while True:\n",
    "    i -= 1\n",
    "    if i <= 0:\n",
    "        flag = False\n",
    "    \n",
    "    \n",
    "    # Get frame at flip it\n",
    "    ret, frame = camera.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    # Get grame frame for face detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detect faces, get coordinate of the left top corner, width and height\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5) \n",
    "    \n",
    "#     for detected_face in faces:\n",
    "#         fx,fy,fw,fh = detected_face\n",
    "    \n",
    "    if not isinstance(faces, tuple):\n",
    "        fx,fy,fw,fh = faces[-1]\n",
    " \n",
    "    # Augment frame with emoji\n",
    "    emoji_img = emoji_dict[current_emoji]\n",
    "    augment_on_frame(frame, emoji_img, 10, 10, 150, 150)\n",
    "    \n",
    "    if cv2.waitKey(5) == 32:\n",
    "        # If space bar is pressed, extract imotion\n",
    "        bytes_string = cv2.imencode('.jpg', frame)[1].tostring()\n",
    "        headers = dict()\n",
    "        headers['Ocp-Apim-Subscription-Key'] = _key\n",
    "        headers['Content-Type'] = 'application/octet-stream'\n",
    "        json = None\n",
    "        params = None\n",
    "        result = processRequest( json, bytes_string, headers, params )\n",
    "        \n",
    "        if result is not None and len(result) > 0:    \n",
    "            # Detect Emoji\n",
    "            data8uint = np.fromstring( bytes_string, np.uint8 ) # Convert string to an unsigned int array\n",
    "            img = cv2.cvtColor( cv2.imdecode( data8uint, cv2.IMREAD_COLOR ), cv2.COLOR_BGR2RGB )\n",
    "            detected_emotion = renderResultOnImage( result, frame )\n",
    "                        \n",
    "            # Check if emoji is matched\n",
    "            if detected_emotion == current_emoji:\n",
    "                guess = \"Correct!\"\n",
    "                correctness_img = util_dict[\"correct\"]\n",
    "                score += 1\n",
    "                previous_emoji = current_emoji\n",
    "            else:\n",
    "                guess = \"Incorrect!\"\n",
    "                correctness_img = util_dict[\"incorrect\"]\n",
    "                previous_emoji = None\n",
    "\n",
    "            # Choose new target emoji\n",
    "            flag = True\n",
    "            current_emoji = random.choice(emoji_dict.keys())\n",
    "            i = 30\n",
    "\n",
    "    elif cv2.waitKey(5) == 27:\n",
    "        break  \n",
    "    \n",
    "    # Add score to the screen\n",
    "    cv2.putText(frame,\"Score: {}\".format(score), (450,50), cv2.FONT_HERSHEY_SIMPLEX, 1,(255,0,0),6)\n",
    "        \n",
    "    # Augment the screen \n",
    "    if flag == True:\n",
    "        # Augment previous emoji on the face\n",
    "        if previous_emoji is not None:\n",
    "            prev_emoji_img = emoji_dict[previous_emoji]\n",
    "            augment_on_frame(frame, prev_emoji_img, int(fx-0.2*fw), int(fy-0.2*fh), int(1.4*fw), int(1.4*fh))\n",
    "        # Add text to the video frame \n",
    "        cv2.putText(frame,\"Detected: {}\".format(detected_emotion), (320,350), cv2.FONT_HERSHEY_SIMPLEX, 1,(255,0,0),4)\n",
    "        augment_on_frame(frame, correctness_img, 400, 360, 120, 120) \n",
    "\n",
    "    # Display the final screen\n",
    "    cv2.imshow(\"Frame\", frame)    \n",
    "cv2.destroyAllWindows()\n",
    "camera.release()\n",
    "cv2.waitKey(1) # extra waitKey sometimes needed to close camera window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
