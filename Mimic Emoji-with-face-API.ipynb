{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import requests\n",
    "import cv2\n",
    "import operator\n",
    "import numpy as np\n",
    "from __future__ import print_function\n",
    "import random\n",
    "\n",
    "from helpers import imshow\n",
    "\n",
    "# Import library to display results\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "# Display images within Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert credentials and initialize Face-API\n",
    "subscription_key = \"\" #Here you have to paste your primary key\n",
    "assert subscription_key\n",
    "face_api_url = 'https://westcentralus.api.cognitive.microsoft.com/face/v1.0/detect'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters required for the request\n",
    "headers = {\n",
    "     'Content-Type': 'application/octet-stream',\n",
    "     'Ocp-Apim-Subscription-Key': subscription_key,\n",
    "}\n",
    "params = {\n",
    "    'returnFaceId': 'false',\n",
    "    'returnFaceLandmarks': 'false',\n",
    "    'returnFaceAttributes': 'emotion',\n",
    "#     'returnFaceAttributes': 'age,gender,headPose,smile,facialHair,glasses,emotion,hair,makeup,occlusion,accessories,blur,exposure,noise',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processRequest(bytes_string, face_api_url = face_api_url, headers = headers, params = params):\n",
    "    response = requests.post(face_api_url, params=params, headers=headers, data = bytes_string)\n",
    "    faces = response.json()\n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmotion(result):        \n",
    "    for currFace in result:\n",
    "        faceRectangle = currFace['faceRectangle']\n",
    "        emotions = currFace['faceAttributes']['emotion']\n",
    "        currEmotion = max(emotions.items(), key=operator.itemgetter(1))[0]\n",
    "    return currEmotion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo Game: Mimic Emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method that imports a library of emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "def importEmoji():\n",
    "    emoji_dict = {}\n",
    "    images = glob.glob('emoji_images/*.jpg')    \n",
    "    for fname in images:\n",
    "        name = fname.split(\"/\")[-1].split(\".\")[0]\n",
    "        emoji_img = cv2.imread(fname, -1)\n",
    "        emoji_img = convert_to_bgra(emoji_img)\n",
    "        emoji_dict[name] = emoji_img\n",
    "    return emoji_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method that imports utility images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importUtil():\n",
    "    util_dict = {}\n",
    "    images = glob.glob('util_img/*.jpg')    \n",
    "    for fname in images:\n",
    "        name = fname.split(\"/\")[-1].split(\".\")[0]\n",
    "        util_img = cv2.imread(fname)\n",
    "        util_img = convert_to_bgra(util_img)\n",
    "        util_dict[name] = util_img\n",
    "    return util_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method that converts image from BGR to BGRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method that converts image from BGR to BGRA\n",
    "def convert_to_bgra(img):\n",
    "    # If image is already BGRA, just return it\n",
    "    if img.shape[2] == 4:\n",
    "        return img\n",
    "    \n",
    "    # Subtract emoji contour\n",
    "    mask = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    ret, mask = cv2.threshold(mask, 230, 255, cv2.THRESH_BINARY_INV)\n",
    "    _, contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    areas = [cv2.contourArea(cont) for cont in contours] # Calculate areas\n",
    "    cnt = contours[np.argmax(areas)] # Only keep the largest contour\n",
    "\n",
    "    mask_cnt = np.zeros(img.shape[:2], np.uint8)\n",
    "    cv2.drawContours(mask_cnt, [cnt], -1, 255, -1)\n",
    "\n",
    "    # Convert to BGRA\n",
    "    b_channel, g_channel, r_channel = cv2.split(img)\n",
    "    alpha_channel = mask_cnt\n",
    "    img_BGRA = cv2.merge((b_channel, g_channel, r_channel, alpha_channel))\n",
    "    return img_BGRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method that augments image on the video frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment image to a frame\n",
    "def augment_on_frame(frame, img, x_offset, y_offset, x_size, y_size):\n",
    "    # set size\n",
    "    img = cv2.resize(img, (x_size,y_size), interpolation = cv2.INTER_AREA)\n",
    "  \n",
    "    # set offset\n",
    "    y1, y2 = y_offset, y_offset + img.shape[0]\n",
    "    x1, x2 = x_offset, x_offset + img.shape[1]\n",
    "\n",
    "    alpha_s = img[:, :, 3] / 255.0\n",
    "    alpha_l = 1.0 - alpha_s\n",
    "    try:\n",
    "        for c in range(0, 3):\n",
    "            frame[y1:y2, x1:x2, c] = (alpha_s * img[:, :, c] +\n",
    "                                      alpha_l * frame[y1:y2, x1:x2, c])\n",
    "    except:\n",
    "        return None\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game - version1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:42: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camera = cv2.VideoCapture(0)\n",
    "# reduce frame size to speed it up\n",
    "w = 640\n",
    "camera.set(cv2.CAP_PROP_FRAME_WIDTH, w) \n",
    "camera.set(cv2.CAP_PROP_FRAME_HEIGHT, w * 3/4) \n",
    "camera.set(cv2.CAP_PROP_EXPOSURE,-4) \n",
    "camera.set(cv2.CAP_PROP_FPS, 24)\n",
    "\n",
    "# Import emoji\n",
    "emoji_dict = importEmoji()\n",
    "current_emoji = random.choice(emoji_dict.keys())\n",
    "\n",
    "# Import utility images\n",
    "util_dict = importUtil()\n",
    "\n",
    "# Initialize score and counter for displaying result information\n",
    "i = 0\n",
    "score = 0\n",
    "\n",
    "while True:\n",
    "    i -= 1\n",
    "    if i <= 0:\n",
    "        flag = False\n",
    "    \n",
    "    \n",
    "    # Get frame at flip it\n",
    "    ret, frame = camera.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    # Augment frame with emoji\n",
    "    emoji_img = emoji_dict[current_emoji]\n",
    "    augment_on_frame(frame, emoji_img, 10, 10, 150, 150)\n",
    "    \n",
    "    if cv2.waitKey(5) == 32:\n",
    "        # If space bar is pressed, extract imotion\n",
    "        bytes_string = cv2.imencode('.jpg', frame)[1].tostring()\n",
    "        result = processRequest(bytes_string)\n",
    "        \n",
    "        \n",
    "        if result is not None and len(result) > 0:    \n",
    "            # Detect Emoji\n",
    "            data8uint = np.fromstring( bytes_string, np.uint8 ) # Convert string to an unsigned int array\n",
    "            img = cv2.cvtColor( cv2.imdecode( data8uint, cv2.IMREAD_COLOR ), cv2.COLOR_BGR2RGB )\n",
    "            detected_emotion = getEmotion( result )\n",
    "                        \n",
    "            # Check if emoji is matched\n",
    "            if detected_emotion == current_emoji:\n",
    "                guess = \"Correct!\"\n",
    "                correctness_img = util_dict[\"correct\"]\n",
    "                score += 1\n",
    "            else:\n",
    "                guess = \"Incorrect!\"\n",
    "                correctness_img = util_dict[\"incorrect\"]\n",
    "\n",
    "            # Display new target emoji\n",
    "        \n",
    "            flag = True\n",
    "            current_emoji = random.choice(emoji_dict.keys())\n",
    "            i = 100\n",
    "\n",
    "    elif cv2.waitKey(5) == 27:\n",
    "        break  \n",
    "    \n",
    "    # Add score to the screen\n",
    "    cv2.putText(frame,\"Score: {}\".format(score), (450,50), cv2.FONT_HERSHEY_SIMPLEX, 1,(255,0,0),6)\n",
    "        \n",
    "    if flag == True:\n",
    "        cv2.putText(frame,\"Detected: {}\".format(detected_emotion), (320,300), cv2.FONT_HERSHEY_SIMPLEX, 1,(255,0,0),4)\n",
    "        augment_on_frame(frame, correctness_img, 400, 310, 120, 120)    \n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    \n",
    "cv2.destroyAllWindows()\n",
    "camera.release()\n",
    "cv2.waitKey(1) # extra waitKey sometimes needed to close camera window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game - version 2 (Face augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-12-0a51e5b40848>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-0a51e5b40848>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    print emoji_dict\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "camera = cv2.VideoCapture(0)\n",
    "# reduce frame size to speed it up\n",
    "w = 640\n",
    "camera.set(cv2.CAP_PROP_FRAME_WIDTH, w) \n",
    "camera.set(cv2.CAP_PROP_FRAME_HEIGHT, w * 3/4) \n",
    "camera.set(cv2.CAP_PROP_EXPOSURE,-4) \n",
    "camera.set(cv2.CAP_PROP_FPS, 24)\n",
    "\n",
    "# Import emoji\n",
    "emoji_dict = importEmoji()\n",
    "current_emoji = random.choice(emoji_dict.keys())\n",
    "\n",
    "# Import utility images\n",
    "util_dict = importUtil()\n",
    "\n",
    "# Import haar cascate for face detection\n",
    "face_cascade = cv2.CascadeClassifier('/usr/local/Cellar/opencv/3.4.0/share/OpenCV/haarcascades/haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Initialize score and counter for displaying result information\n",
    "i = 0\n",
    "score = 0\n",
    "\n",
    "while True:\n",
    "    i -= 1\n",
    "    if i <= 0:\n",
    "        flag = False\n",
    "    \n",
    "    \n",
    "    # Get frame at flip it\n",
    "    ret, frame = camera.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    # Get grame frame for face detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detect faces, get coordinate of the left top corner, width and height\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5) \n",
    "    \n",
    "#     for detected_face in faces:\n",
    "#         fx,fy,fw,fh = detected_face\n",
    "    \n",
    "    if not isinstance(faces, tuple):\n",
    "        fx,fy,fw,fh = faces[-1]\n",
    " \n",
    "    # Augment frame with emoji\n",
    "    emoji_img = emoji_dict[current_emoji]\n",
    "    augment_on_frame(frame, emoji_img, 10, 10, 150, 150)\n",
    "    \n",
    "    if cv2.waitKey(5) == 32:\n",
    "        # If space bar is pressed, extract imotion\n",
    "        bytes_string = cv2.imencode('.jpg', frame)[1].tostring()\n",
    "        result = processRequest(bytes_string)\n",
    "        \n",
    "        if result is not None and len(result) > 0:    \n",
    "            # Detect Emoji\n",
    "            data8uint = np.fromstring( bytes_string, np.uint8 ) # Convert string to an unsigned int array\n",
    "            img = cv2.cvtColor( cv2.imdecode( data8uint, cv2.IMREAD_COLOR ), cv2.COLOR_BGR2RGB )\n",
    "            detected_emotion = getEmotion(result)\n",
    "                        \n",
    "            # Check if emoji is matched\n",
    "            if detected_emotion == current_emoji:\n",
    "                guess = \"Correct!\"\n",
    "                correctness_img = util_dict[\"correct\"]\n",
    "                score += 1\n",
    "                previous_emoji = current_emoji\n",
    "            else:\n",
    "                guess = \"Incorrect!\"\n",
    "                correctness_img = util_dict[\"incorrect\"]\n",
    "                previous_emoji = None\n",
    "\n",
    "            # Choose new target emoji\n",
    "            flag = True\n",
    "            current_emoji = random.choice(emoji_dict.keys())\n",
    "            i = 30\n",
    "\n",
    "    elif cv2.waitKey(5) == 27:\n",
    "        break  \n",
    "    \n",
    "    # Add score to the screen\n",
    "    cv2.putText(frame,\"Score: {}\".format(score), (450,50), cv2.FONT_HERSHEY_SIMPLEX, 1,(255,0,0),6)\n",
    "        \n",
    "    # Augment the screen \n",
    "    if flag == True:\n",
    "        # Augment previous emoji on the face\n",
    "        if previous_emoji is not None:\n",
    "            prev_emoji_img = emoji_dict[previous_emoji]\n",
    "            augment_on_frame(frame, prev_emoji_img, int(fx-0.2*fw), int(fy-0.2*fh), int(1.4*fw), int(1.4*fh))\n",
    "        # Add text to the video frame \n",
    "        cv2.putText(frame,\"Detected: {}\".format(detected_emotion), (320,350), cv2.FONT_HERSHEY_SIMPLEX, 1,(255,0,0),4)\n",
    "        augment_on_frame(frame, correctness_img, 400, 360, 120, 120) \n",
    "\n",
    "    # Display the final screen\n",
    "    cv2.imshow(\"Frame\", frame)    \n",
    "cv2.destroyAllWindows()\n",
    "camera.release()\n",
    "cv2.waitKey(1) # extra waitKey sometimes needed to close camera window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:60: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camera = cv2.VideoCapture(0)\n",
    "# reduce frame size to speed it up\n",
    "w = 640\n",
    "camera.set(cv2.CAP_PROP_FRAME_WIDTH, w) \n",
    "camera.set(cv2.CAP_PROP_FRAME_HEIGHT, w * 3/4) \n",
    "camera.set(cv2.CAP_PROP_EXPOSURE,-4) \n",
    "camera.set(cv2.CAP_PROP_FPS, 24)\n",
    "\n",
    "# Import emoji\n",
    "emoji_dict = importEmoji()\n",
    "use_for_test = [5, 2, 7]\n",
    "\n",
    "em = 0\n",
    "\n",
    "current_emoji = emoji_dict.keys()[use_for_test[em]]\n",
    "\n",
    "# Import utility images\n",
    "util_dict = importUtil()\n",
    "\n",
    "# Import haar cascate for face detection\n",
    "face_cascade = cv2.CascadeClassifier('/usr/local/Cellar/opencv/3.4.0/share/OpenCV/haarcascades/haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Initialize score and counter for displaying result information\n",
    "i = 0\n",
    "score = 0\n",
    "\n",
    "while True:\n",
    "    i -= 1\n",
    "    if i <= 0:\n",
    "        flag = False\n",
    "    \n",
    "    \n",
    "    # Get frame at flip it\n",
    "    ret, frame = camera.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    # Get grame frame for face detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detect faces, get coordinate of the left top corner, width and height\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5) \n",
    "    \n",
    "#     for detected_face in faces:\n",
    "#         fx,fy,fw,fh = detected_face\n",
    "    \n",
    "    if not isinstance(faces, tuple):\n",
    "        fx,fy,fw,fh = faces[-1]\n",
    " \n",
    "    # Augment frame with emoji\n",
    "    emoji_img = emoji_dict[current_emoji]\n",
    "    augment_on_frame(frame, emoji_img, 10, 10, 150, 150)\n",
    "    \n",
    "    if cv2.waitKey(5) == 32:\n",
    "        # If space bar is pressed, extract imotion\n",
    "        bytes_string = cv2.imencode('.jpg', frame)[1].tostring()\n",
    "        result = processRequest(bytes_string)\n",
    "        \n",
    "        if result is not None and len(result) > 0:    \n",
    "            # Detect Emoji\n",
    "            data8uint = np.fromstring( bytes_string, np.uint8 ) # Convert string to an unsigned int array\n",
    "            img = cv2.cvtColor( cv2.imdecode( data8uint, cv2.IMREAD_COLOR ), cv2.COLOR_BGR2RGB )\n",
    "            detected_emotion = getEmotion(result)\n",
    "                        \n",
    "            # Check if emoji is matched\n",
    "            if detected_emotion == current_emoji:\n",
    "                guess = \"Correct!\"\n",
    "                correctness_img = util_dict[\"correct\"]\n",
    "                score += 1\n",
    "                previous_emoji = current_emoji\n",
    "            else:\n",
    "                guess = \"Incorrect!\"\n",
    "                correctness_img = util_dict[\"incorrect\"]\n",
    "                previous_emoji = None\n",
    "\n",
    "            # Choose new target emoji\n",
    "            flag = True\n",
    "            em = (em+1)%3\n",
    "            current_emoji = emoji_dict.keys()[use_for_test[em]]\n",
    "\n",
    "            i = 30\n",
    "\n",
    "    elif cv2.waitKey(5) == 27:\n",
    "        break  \n",
    "    \n",
    "    # Add score to the screen\n",
    "    cv2.putText(frame,\"Score: {}\".format(score), (450,50), cv2.FONT_HERSHEY_SIMPLEX, 1,(255,0,0),6)\n",
    "        \n",
    "    # Augment the screen \n",
    "    if flag == True:\n",
    "        # Augment previous emoji on the face\n",
    "        if previous_emoji is not None:\n",
    "            prev_emoji_img = emoji_dict[previous_emoji]\n",
    "            augment_on_frame(frame, prev_emoji_img, int(fx-0.2*fw), int(fy-0.2*fh), int(1.4*fw), int(1.4*fh))\n",
    "        # Add text to the video frame \n",
    "        cv2.putText(frame,\"Detected: {}\".format(detected_emotion), (320,350), cv2.FONT_HERSHEY_SIMPLEX, 1,(255,0,0),4)\n",
    "        augment_on_frame(frame, correctness_img, 400, 360, 120, 120) \n",
    "\n",
    "    # Display the final screen\n",
    "    cv2.imshow(\"Frame\", frame)    \n",
    "cv2.destroyAllWindows()\n",
    "camera.release()\n",
    "cv2.waitKey(1) # extra waitKey sometimes needed to close camera window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
